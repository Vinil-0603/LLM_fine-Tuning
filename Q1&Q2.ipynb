{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from datasets import load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load your data from a CSV file\n",
    "# Replace 'your_data.csv' with the actual file path to your CSV file\n",
    "data = pd.read_csv('/Users/saivinilreddy/Downloads/SMA-Q1 &Q2/test.csv')\n",
    "\n",
    "# Create a dataset from the CSV data\n",
    "dataset = Dataset.from_pandas(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# Load the pre-trained model and tokenizer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-large\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "\n",
    "# Function to generate outputs for long input text by dividing it into chunks\n",
    "def generate_output(input_text):\n",
    "    max_chunk_length = 500\n",
    "    input_chunks = [input_text[i:i + max_chunk_length] for i in range(0, len(input_text), max_chunk_length)]\n",
    "    output_chunks = []\n",
    "\n",
    "    for chunk in input_chunks:\n",
    "        inputs = tokenizer(chunk, return_tensors=\"pt\")\n",
    "        outputs = model.generate(**inputs, max_length=10000)\n",
    "        decoded_output = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        output_chunks.append(decoded_output[0])\n",
    "\n",
    "    return \"\".join(output_chunks)\n",
    "\n",
    "# Read input from CSV and write outputs to CSV in chunks of 50 rows\n",
    "input_csv_file = \"test.csv\"\n",
    "output_csv_file = \"output.csv\"\n",
    "batch_size = 5\n",
    "\n",
    "with open(input_csv_file, \"r\", encoding=\"utf-8\") as csvfile, open(output_csv_file, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile_out:\n",
    "    reader = csv.reader(csvfile)\n",
    "    writer = csv.writer(csvfile_out)\n",
    "\n",
    "    # Write header to output CSV file if needed\n",
    "    # writer.writerow([\"Output\"])\n",
    "\n",
    "    input_batch = []\n",
    "    for idx, row in enumerate(reader, 1):\n",
    "        input_text = row[0]  # Assuming the input is in the first column of the CSV\n",
    "        input_batch.append(input_text)\n",
    "\n",
    "        # Process batch of 50 rows at a time\n",
    "        if idx % batch_size == 0 or idx == batch_size:\n",
    "            output_batch = [generate_output(text) for text in input_batch]\n",
    "            for output_text in output_batch:\n",
    "                writer.writerow([output_text])\n",
    "            input_batch = []\n",
    "\n",
    "print(\"Outputs generated and saved to output.csv.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Cumulative BERT Scores:\n",
      "Precision: 0.7750833320617676\n",
      "Recall: 0.7775522385835647\n",
      "F1 Score: 0.7750406132936478\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from evaluate import load\n",
    "\n",
    "# Load the BERT score model\n",
    "bertscore = load(\"bertscore\")\n",
    "\n",
    "# Read data from CSV file\n",
    "df = pd.read_csv(\"final.csv\")\n",
    "\n",
    "# Extract actual and predicted values from the DataFrame\n",
    "predictions = df[\"Predicted\"].tolist()\n",
    "references = df[\"Actual\"].tolist()\n",
    "\n",
    "# Compute BERT scores for each pair of actual and predicted values\n",
    "results = bertscore.compute(predictions=predictions, references=references, model_type=\"distilbert-base-uncased\")\n",
    "\n",
    "# Extract individual BERT scores (precision, recall, F1) for each pair\n",
    "precision = results[\"precision\"]\n",
    "recall = results[\"recall\"]\n",
    "f1 = results[\"f1\"]\n",
    "\n",
    "# Create a new DataFrame to store the BERT scores\n",
    "output_df = pd.DataFrame({\"Actual\": references, \"Predicted\": predictions, \"Precision\": precision, \"Recall\": recall, \"F1\": f1})\n",
    "\n",
    "# Save the BERT scores to a new CSV file\n",
    "output_df.to_csv(\"bert_scores_output.csv\", index=False)\n",
    "\n",
    "# Calculate the final cumulative BERT scores\n",
    "final_precision = sum(precision) / len(precision)\n",
    "final_recall = sum(recall) / len(recall)\n",
    "final_f1 = sum(f1) / len(f1)\n",
    "\n",
    "# Print the final cumulative BERT scores\n",
    "print(\"Final Cumulative BERT Scores:\")\n",
    "print(\"Precision:\", final_precision)\n",
    "print(\"Recall:\", final_recall)\n",
    "print(\"F1 Score:\", final_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c6cc169ecc342d8bdffbf944b26d252",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE Scores:\n",
      "ROUGE-1: 0.2690847700213629\n",
      "ROUGE-2: 0.09000883228910794\n",
      "ROUGE-L: 0.1785974257179503\n",
      "ROUGE-Lsum: 0.17822636563720617\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from evaluate import load\n",
    "\n",
    "# Load the ROUGE score model\n",
    "rouge = load('rouge')\n",
    "\n",
    "# Read data from CSV file\n",
    "df = pd.read_csv(\"final.csv\")\n",
    "\n",
    "# Extract actual and predicted values from the DataFrame\n",
    "predictions = df[\"Predicted\"].tolist()\n",
    "references = df[\"Actual\"].tolist()\n",
    "\n",
    "# Compute ROUGE scores for each pair of actual and predicted values\n",
    "results = rouge.compute(predictions=predictions, references=references)\n",
    "\n",
    "# Save the ROUGE scores to a new CSV file\n",
    "output_df = pd.DataFrame({\"Actual\": references, \"Predicted\": predictions, \"ROUGE-1\": results['rouge1'], \"ROUGE-2\": results['rouge2'], \"ROUGE-L\": results['rougeL'], \"ROUGE-Lsum\": results['rougeLsum']})\n",
    "\n",
    "output_df.to_csv(\"rouge_scores_output.csv\", index=False)\n",
    "\n",
    "# Print the ROUGE scores\n",
    "print(\"ROUGE Scores:\")\n",
    "print(\"ROUGE-1:\", results['rouge1'])\n",
    "print(\"ROUGE-2:\", results['rouge2'])\n",
    "print(\"ROUGE-L:\", results['rougeL'])\n",
    "print(\"ROUGE-Lsum:\", results['rougeLsum'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/saivinilreddy/anaconda3/envs/GPU/lib/python3.11/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BLEU Score: 0.29171001991417306\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from evaluate import load\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "# Load BLEU score function from NLTK\n",
    "def compute_bleu_score(prediction, references):\n",
    "    return sentence_bleu(references, prediction)\n",
    "\n",
    "# Read data from CSV file\n",
    "df = pd.read_csv(\"final.csv\")\n",
    "\n",
    "# Extract actual and predicted values from the DataFrame\n",
    "predictions = df[\"Predicted\"].tolist()\n",
    "references_list = df[\"Actual\"].apply(lambda x: [reference.strip() for reference in x.split(',')]).tolist()\n",
    "\n",
    "# Compute BLEU scores for each pair of actual and predicted values\n",
    "bleu_scores = [compute_bleu_score(prediction, references) for prediction, references in zip(predictions, references_list)]\n",
    "\n",
    "# Create a new DataFrame to store the BLEU scores\n",
    "output_df = pd.DataFrame({\"Actual\": df[\"Actual\"], \"Predicted\": predictions, \"BLEU Score\": bleu_scores})\n",
    "\n",
    "# Save the BLEU scores to a new CSV file\n",
    "output_df.to_csv(\"bleu_scores_output.csv\", index=False)\n",
    "\n",
    "# Calculate the average BLEU score\n",
    "average_bleu_score = sum(bleu_scores) / len(bleu_scores)\n",
    "\n",
    "# Print the average BLEU score\n",
    "print(\"Average BLEU Score:\", average_bleu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
